{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.006895,
     "end_time": "2020-09-06T02:15:41.910994",
     "exception": false,
     "start_time": "2020-09-06T02:15:41.904099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extracting & Plotting  Feature Names & Importance from Scikit-Learn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.004358,
     "end_time": "2020-09-06T02:15:41.921174",
     "exception": false,
     "start_time": "2020-09-06T02:15:41.916816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you have ever been tasked with productionalizing a machine learning model, you probably know that Scikit-Learn library offers one of the best ways -- if not the best way -- of creating production-quality machine learning workflows. The ecosystem's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), [preprocessors](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing), [imputers](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute) & [feature selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) classes are powerful tools that transform raw data into model-ready features.\n",
    "\n",
    "However, before anyone is going to let you deploy to production, you are going to want to have some minimal understanding of how the new model works. The most common way to explain how a black-box model works is by plotting feature names and importance values. If you have ever tried to extract the feature names from a heterogeneous dataset processed by ColumnTransformer, you know that this is no easy task. Exhaustive Internet searches have only brought to my attention where others have [asked](https://github.com/scikit-learn/scikit-learn/issues/6424) [the](https://github.com/scikit-learn/scikit-learn/pull/6431) [same](https://github.com/scikit-learn/scikit-learn/pull/12627) [question](https://github.com/scikit-learn/scikit-learn/pull/13307) or offered a [partial answer](https://github.com/scikit-learn/scikit-learn/issues/12525), instead of yielding a comprehensive and satisfying solution. \n",
    "\n",
    "To remedy this situation, I have developed a class called `FeatureImportance` that will extract feature names and importance values from a Pipeline instance. It then uses the Plotly library to plot the feature importance using only a few lines of code. In this post, I will load a fitted Pipeline, demonstrate how to use my class and then give an overview of how it works. The complete code can be found [here](https://www.kaggle.com/kylegilde/feature-importance) or at the end of this blog post.\n",
    "\n",
    "There are two things I should note before continuing:\n",
    "\n",
    "1. I credit Joey Gao's code on [this thread](https://github.com/scikit-learn/scikit-learn/issues/12525#issuecomment-436217100) with showing the way to tackle this problem.\n",
    "\n",
    "2. My post assumes that you have worked with Scikit-Learn and Pandas before and are familiar with how ColumnTransformer, Pipeline & preprocessing classes facilitate reproducible feature engineering processes. If you need a refresher, check out this [Scikit-Learn example](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html).\n",
    "\n",
    "## Creating a Pipeline\n",
    "\n",
    "\n",
    "For the purposes of demonstration, I've written a script called [fit_pipeline_ames.py](https://www.kaggle.com/kylegilde/fit-pipeline-ames). It loads the [Ames housing training data from Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) and fits a moderately complex Pipeline. The `pipe` instance contains the following 4 steps:\n",
    "\n",
    "1. The [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) instance is composed of 3 Pipelines, containing a total of 4 transformer instances, including [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html), [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) & [GLMMEncoder](http://contrib.scikit-learn.org/category_encoders/glmm.html) from the [category_encoders](https://contrib.scikit-learn.org/category_encoders/) package. See my [previous blog post](https://towardsdatascience.com/building-columntransformers-dynamically-1-6354bd08aa54) for a full explanation of how I dynamically constructed this particular ColumnTransformer.\n",
    "\n",
    "2. The [VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) uses the default threshold of 0, which removes any features that contain only a single value. Some models will fail if a feature has no variance.\n",
    "\n",
    "3. The [SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html) uses the [f_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html) scoring function with a percentile threshold of 90. These settings retain the top 90% of features and discard the bottom 10%.\n",
    "\n",
    "4. The [CatBoostRegressor](https://catboost.ai/docs/concepts/python-reference_catboostregressor.html) model is fit to the `SalesPrice` dependent variable using the features created and selected in the preceding steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-06T02:15:41.935858Z",
     "iopub.status.busy": "2020-09-06T02:15:41.935011Z",
     "iopub.status.idle": "2020-09-06T02:15:51.816527Z",
     "shell.execute_reply": "2020-09-06T02:15:51.817232Z"
    },
    "papermill": {
     "duration": 9.891583,
     "end_time": "2020-09-06T02:15:51.817427",
     "exception": false,
     "start_time": "2020-09-06T02:15:41.925844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('column_transformer',\n",
       "                 ColumnTransformer(n_jobs=4,\n",
       "                                   transformers=[('numeric_pipeline',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='median'))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7fd44216a9d0>),\n",
       "                                                 ('oh_pipeline',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='constant'...\n",
       "                                                  <function select_oh_features at 0x7fd43ff814d0>),\n",
       "                                                 ('hc_pipeline',\n",
       "                                                  Pipeline(steps=[('glmmencoder',\n",
       "                                                                   GLMMEncoder())]),\n",
       "                                                  <function select_hc_features at 0x7fd44bb17050>)])),\n",
       "                ('variancethreshold', VarianceThreshold()),\n",
       "                ('selectpercentile',\n",
       "                 SelectPercentile(percentile=90,\n",
       "                                  score_func=<function f_regression at 0x7fd440321710>)),\n",
       "                ('model',\n",
       "                 <catboost.core.CatBoostRegressor object at 0x7fd435f3cbd0>)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fit_pipeline_ames import *\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.004687,
     "end_time": "2020-09-06T02:15:51.827258",
     "exception": false,
     "start_time": "2020-09-06T02:15:51.822571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plotting FeatureImportance\n",
    "\n",
    "\n",
    "With the help of FeatureImportance, we can extract the feature names and importance values and plot them with 3 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-06T02:15:51.843599Z",
     "iopub.status.busy": "2020-09-06T02:15:51.842742Z",
     "iopub.status.idle": "2020-09-06T02:15:53.361996Z",
     "shell.execute_reply": "2020-09-06T02:15:53.362587Z"
    },
    "papermill": {
     "duration": 1.530521,
     "end_time": "2020-09-06T02:15:53.362739",
     "exception": false,
     "start_time": "2020-09-06T02:15:51.832218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"55317e0d-cea1-4de8-8023-e3f8f462c599\" class=\"plotly-graph-div\" style=\"height:625px; width:750px;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"55317e0d-cea1-4de8-8023-e3f8f462c599\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '55317e0d-cea1-4de8-8023-e3f8f462c599',\n",
       "                        [{\"alignmentgroup\": \"True\", \"hovertemplate\": \"value=%{x}<br>feature=%{y}<br>text=%{text}<extra></extra>\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"h\", \"showlegend\": false, \"text\": [4.1, 4.7, 4.8, 5.1, 5.2, 5.2, 6.0, 6.9, 7.0, 7.3, 8.4, 10.9, 11.5, 12.0, 12.2, 12.9, 13.9, 14.7, 16.8, 24.8, 25.0, 33.4, 54.8, 81.3, 100.0], \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [4.057716514813632, 4.711673423790712, 4.782090824644379, 5.055290599510618, 5.152162919776967, 5.173599199799967, 6.041117957590613, 6.89747162133185, 7.000911272842426, 7.3459747681385235, 8.408865850020373, 10.920415923193257, 11.469447291515268, 12.036818275165148, 12.168626109110223, 12.93205764184344, 13.906600190369842, 14.708000352393185, 16.787887128582092, 24.796964684580736, 25.015267340501623, 33.36820438540082, 54.763100620997406, 81.33363623337158, 100.0], \"xaxis\": \"x\", \"y\": [\"25.      WoodDeckSF\", \"24.     BsmtQual_Ex\", \"23.    CentralAir_N\", \"22. BsmtFinType1_GLQ\", \"21.  KitchenQual_Ex\", \"20.     LotFrontage\", \"19.  KitchenQual_TA\", \"18.    CentralAir_Y\", \"17.     GarageYrBlt\", \"16.        2ndFlrSF\", \"15.    TotRmsAbvGrd\", \"14.      GarageCars\", \"13.       YearBuilt\", \"12.      Fireplaces\", \"11. FireplaceQu_missing_value\", \"10.      BsmtFinSF1\", \"9.      GarageArea\", \"8.    YearRemodAdd\", \"7.     OverallCond\", \"6.         LotArea\", \"5.        1stFlrSF\", \"4.     TotalBsmtSF\", \"3.    Neighborhood\", \"2.       GrLivArea\", \"1.     OverallQual\"], \"yaxis\": \"y\"}],\n",
       "                        {\"barmode\": \"relative\", \"height\": 625, \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Top 25 (of 228) Feature Importances\", \"x\": 0.5}, \"width\": 750, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"value\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"tickfont\": {\"family\": \"Courier New\", \"size\": 15}, \"title\": {\"text\": \"\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('55317e0d-cea1-4de8-8023-e3f8f462c599');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from feature_importance import FeatureImportance\n",
    "feature_importance = FeatureImportance(pipe)\n",
    "feature_importance.plot(top_n_features=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.005131,
     "end_time": "2020-09-06T02:15:53.373480",
     "exception": false,
     "start_time": "2020-09-06T02:15:53.368349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `plot` method takes a number of arguments that control the plot's display. The most important ones are the following:\n",
    "\n",
    "- `top_n_features`: This controls how many features will be plotted. The default value is 100. The plot's title will indicate this value as well as how many features there are in total. To plot all features, just set `top_n_features` to a number larger than the total features. \n",
    "\n",
    "- `rank_features`: This argument controls whether the integer ranks are displayed in front of the feature names. The default is `True`. I find that this aids with interpretation, especially when comparing the feature importance from multiple models.\n",
    "\n",
    "- `max_scale`: This determines whether the importance values are scaled by the maximum value & multiplied by 100. The default is `True`. I find that this enables an intuitive way to compare how important other features are vis-a-viz the most important one. For instance, in the plot of above, we can say that `GrLivArea` is about 81% as important to the model as the top feature, `OverallQty`."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.005119,
     "end_time": "2020-09-06T02:15:53.383953",
     "exception": false,
     "start_time": "2020-09-06T02:15:53.378834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## How It Works\n",
    "\n",
    "The `FeatureImportance` class should be instantiated using a fitted Pipeline instance. (You can also change the `verbose` argument to `True` if you want to have all of the diagnostics printed to your console.) My class validates that this Pipeline starts with a `ColumnTransformer` instance and ends with a regression or classification model that has the `feature_importance_` attribute. As intermediate steps, the Pipeline can have any number or no instances of classes from [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "The `FeatureImportance` class is composed of 4 methods.\n",
    "\n",
    "1. `get_feature_names_from_col_transformer` was the hardest method to devise. It iterates through the `ColumnTransformer` transformers, uses the `hasattr` function to discern what type of class we are dealing with and pulls the feature names accordingly. (Special Note: If the ColumnTransformer contains Pipelines and if one of the transformers in the Pipeline is adding completely new columns, it must come last in the pipeline. For example, OneHotEncoder, [MissingIndicator](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html) & SimpleImputer(add_indicator=True) add columns to the dataset that didn't exist before, so they should come last in the Pipeline.)\n",
    "\n",
    "2. `get_selected_features` calls `get_feature_names_from_col_transformer`. Then it tests for whether the main Pipeline contains any classes from sklearn.feature_selection based upon the existence of the `get_support` method. If it does, this method returns only the features names that were retained by the selector class or classes.\n",
    "\n",
    "3. `get_feature_importance` calls `get_selected_features` and then creates a Pandas Series where values are the feature importance values from the model and its index is the feature names created by the first 2 methods. This Series is then stored in the `feature_importance` attribute.\n",
    "\n",
    "4. `plot` calls `get_feature_importance` and plots the output based upon the specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.004966,
     "end_time": "2020-09-06T02:15:53.394595",
     "exception": false,
     "start_time": "2020-09-06T02:15:53.389629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Complete Code\n",
    "\n",
    "The complete code is shown below and can be found here. If you create a Pipeline that you believe should be supported by FeatureImportance but is not, please provide a reproducible example, and I will consider making the necessary changes. \n",
    "\n",
    "The original notebook for this blog post can be found [here](https://www.kaggle.com/kylegilde/extracting-scikit-feature-names-importances). Stay tuned for further posts on training & regularizing models with Scikit-Learn ColumnTransformers and Pipelines. Let me know if you found this post helpful or have any ideas for improvement. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-06T02:15:53.441402Z",
     "iopub.status.busy": "2020-09-06T02:15:53.436017Z",
     "iopub.status.idle": "2020-09-06T02:15:53.469670Z",
     "shell.execute_reply": "2020-09-06T02:15:53.468829Z"
    },
    "papermill": {
     "duration": 0.069931,
     "end_time": "2020-09-06T02:15:53.469804",
     "exception": false,
     "start_time": "2020-09-06T02:15:53.399873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "class FeatureImportance:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract & Plot the Feature Names & Importance Values from a Scikit-Learn Pipeline.\n",
    "    \n",
    "    The input is a Pipeline that starts with a ColumnTransformer & ends with a regression or classification model. \n",
    "    As intermediate steps, the Pipeline can have any number or no instances from sklearn.feature_selection.\n",
    "\n",
    "    Note: \n",
    "    If the ColumnTransformer contains Pipelines and if one of the transformers in the Pipeline is adding completely new columns, \n",
    "    it must come last in the pipeline. For example, OneHotEncoder, MissingIndicator & SimpleImputer(add_indicator=True) add columns \n",
    "    to the dataset that didn't exist before, so there should come last in the Pipeline.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : a Scikit-learn Pipeline class where the a ColumnTransformer is the first element and model estimator is the last element\n",
    "    verbose : a boolean. Whether to print all of the diagnostics. Default is False.\n",
    "    \n",
    "    Attributes\n",
    "    __________\n",
    "    \n",
    "    feature_importance :  A Pandas Series containing the feature importance values and feature names as the index.    \n",
    "    discarded_features : The features names that were not selected by a sklearn.feature_selection instance.\n",
    "    plot_importances_dt : A Pandas DataFrame containing the subset of features and values that are actually displaced in the plot. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline, verbose=False):\n",
    "        self.pipeline = pipeline\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "    def get_feature_names_from_col_transformer(self, verbose=None):  \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Get the column names from the a ColumnTransformer containing transformers & pipelines\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : a boolean indicating whether to print summaries. \n",
    "            default = False\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a list of the correct feature names\n",
    "\n",
    "        Note: \n",
    "        If the ColumnTransformer contains Pipelines and if one of the transformers in the Pipeline is adding completely new columns, \n",
    "        it must come last in the pipeline. For example, OneHotEncoder, MissingIndicator & SimpleImputer(add_indicator=True) add columns \n",
    "        to the dataset that didn't exist before, so there should come last in the Pipeline.\n",
    "\n",
    "        Inspiration: https://github.com/scikit-learn/scikit-learn/issues/12525 \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if verbose is None:\n",
    "            verbose = self.verbose\n",
    "            \n",
    "        if verbose: print('''\\n\\n---------\\nRunning get_feature_names_from_col_transformer\\n---------\\n''')\n",
    "        \n",
    "        column_transformer = self.pipeline[0]        \n",
    "        assert isinstance(column_transformer, ColumnTransformer), \"Input isn't a ColumnTransformer\"\n",
    "        check_is_fitted(column_transformer)\n",
    "\n",
    "        new_feature_names = []\n",
    "\n",
    "        for i, transformer_item in enumerate(column_transformer.transformers_): \n",
    "            \n",
    "            transformer_name, transformer, orig_feature_names = transformer_item\n",
    "            orig_feature_names = list(orig_feature_names)\n",
    "            \n",
    "            if verbose: \n",
    "                print('\\n\\n', i, '. Transformer/Pipeline: ', transformer_name, ',', \n",
    "                      transformer.__class__.__name__, '\\n')\n",
    "                print('\\tn_orig_feature_names:', len(orig_feature_names))\n",
    "\n",
    "            if transformer_name == 'remainder' and transformer == 'drop':\n",
    "                    \n",
    "                continue\n",
    "                \n",
    "            if isinstance(transformer, Pipeline):\n",
    "                # if pipeline, get the last transformer in the Pipeline\n",
    "                transformer = transformer.steps[-1][1]\n",
    "\n",
    "            if hasattr(transformer, 'get_feature_names'):\n",
    "\n",
    "                if 'input_features' in transformer.get_feature_names.__code__.co_varnames:\n",
    "\n",
    "                    names = list(transformer.get_feature_names(orig_feature_names))\n",
    "\n",
    "                else:\n",
    "\n",
    "                    names = list(transformer.get_feature_names())\n",
    "\n",
    "            elif hasattr(transformer,'indicator_') and transformer.add_indicator:\n",
    "                # is this transformer one of the imputers & did it call the MissingIndicator?\n",
    "\n",
    "                missing_indicator_indices = transformer.indicator_.features_\n",
    "                missing_indicators = [orig_feature_names[idx] + '_missing_flag'\\\n",
    "                                      for idx in missing_indicator_indices]\n",
    "                names = orig_feature_names + missing_indicators\n",
    "\n",
    "            elif hasattr(transformer,'features_'):\n",
    "                # is this a MissingIndicator class? \n",
    "                missing_indicator_indices = transformer.features_\n",
    "                missing_indicators = [orig_feature_names[idx] + '_missing_flag'\\\n",
    "                                      for idx in missing_indicator_indices]\n",
    "\n",
    "            else:\n",
    "\n",
    "                names = orig_feature_names\n",
    "\n",
    "            if verbose: \n",
    "                print('\\tn_new_features:', len(names))\n",
    "                print('\\tnew_features:\\n', names)\n",
    "\n",
    "            new_feature_names.extend(names)\n",
    "\n",
    "        return new_feature_names\n",
    "\n",
    "    \n",
    "    def get_selected_features(self, verbose=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Get the Feature Names that were retained after Feature Selection (sklearn.feature_selection)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : a boolean indicating whether to print summaries. default = False\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a list of the correct feature names\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if verbose is None:\n",
    "            verbose = self.verbose\n",
    "\n",
    "        assert isinstance(self.pipeline, Pipeline), \"Input isn't a Pipeline\"\n",
    "\n",
    "        features = self.get_feature_names_from_col_transformer()\n",
    "        \n",
    "        if verbose: print('\\n\\n---------\\nRunning get_selected_features\\n---------\\n')\n",
    "            \n",
    "        all_discarded_features = []\n",
    "\n",
    "        for i, step_item in enumerate(self.pipeline.steps[:]):\n",
    "            \n",
    "            step_name, step = step_item\n",
    "\n",
    "            if hasattr(step, 'get_support'):\n",
    "\n",
    "                if verbose: print('\\nStep ', i, \": \", step_name, ',', \n",
    "                                  step.__class__.__name__, '\\n')\n",
    "                    \n",
    "                check_is_fitted(step)\n",
    "\n",
    "                feature_mask = step.get_support()\n",
    "                features = [feature for feature, is_retained in zip(features, feature_mask)\\\n",
    "                            if is_retained]\n",
    "                discarded_features = [feature for feature, is_retained in zip(features, feature_mask)\\\n",
    "                                      if not is_retained]\n",
    "                all_discarded_features.extend(discarded_features)\n",
    "                \n",
    "                if verbose: \n",
    "                    print(f'\\t{len(features)} retained, {len(discarded_features)} discarded')\n",
    "                    if len(discarded_features) > 0:\n",
    "                        print('\\n\\tdiscarded_features:\\n\\n', discarded_features)\n",
    "\n",
    "        self.discarded_features = all_discarded_features\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a Pandas Series where values are the feature importance values from the model and feature names are set as the index. \n",
    "        \n",
    "        This Series is stored in the `feature_importance` attribute.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A pandas Series containing the feature importance values and feature names as the index.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        assert isinstance(self.pipeline, Pipeline), \"Input isn't a Pipeline\"\n",
    "\n",
    "        features = self.get_selected_features()\n",
    "             \n",
    "        assert hasattr(self.pipeline[-1], 'feature_importances_'),\\\n",
    "            \"The last element in the pipeline isn't an estimator with a feature_importances_ attribute\"\n",
    "        \n",
    "        importance_values = self.pipeline[-1].feature_importances_\n",
    "        \n",
    "        assert len(features) == len(importance_values),\\\n",
    "            \"The number of feature names & importance values doesn't match\"\n",
    "        \n",
    "        feature_importance = pd.Series(importance_values, index=features)\n",
    "        self.feature_importance = feature_importance\n",
    "        \n",
    "        return feature_importance\n",
    "        \n",
    "    \n",
    "    def plot(self, top_n_features=100, rank_features=True, max_scale=True, \n",
    "             display_imp_values=True, display_imp_value_decimals=1,\n",
    "             height_per_feature=25, orientation='h', width=750, height=None, \n",
    "             str_pad_width=15, yaxes_tickfont_family='Courier New', \n",
    "             yaxes_tickfont_size=15):\n",
    "        \"\"\"\n",
    "\n",
    "        Plot the Feature Names & Importances \n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        top_n_features : the number of features to plot, default is 100\n",
    "        rank_features : whether to rank the features with integers, default is True\n",
    "        max_scale : Should the importance values be scaled by the maximum value & mulitplied by 100?  Default is True.\n",
    "        display_imp_values : Should the importance values be displayed? Default is True.\n",
    "        display_imp_value_decimals : If display_imp_values is True, how many decimal places should be displayed. Default is 1.\n",
    "        height_per_feature : if height is None, the plot height is calculated by top_n_features * height_per_feature. \n",
    "        This allows all the features enough space to be displayed\n",
    "        orientation : the plot orientation, 'h' (default) or 'v'\n",
    "        width :  the width of the plot, default is 500\n",
    "        height : the height of the plot, the default is top_n_features * height_per_feature\n",
    "        str_pad_width : When rank_features=True, this number of spaces to add between the rank integer and feature name. \n",
    "            This will enable the rank integers to line up with each other for easier reading. \n",
    "            Default is 15. If you have long feature names, you can increase this number to make the integers line up more.\n",
    "            It can also be set to 0.\n",
    "        yaxes_tickfont_family : the font for the feature names. Default is Courier New.\n",
    "        yaxes_tickfont_size : the font size for the feature names. Default is 15.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        plot\n",
    "\n",
    "        \"\"\"\n",
    "        if height is None:\n",
    "            height = top_n_features * height_per_feature\n",
    "            \n",
    "        # prep the data\n",
    "        \n",
    "        all_importances = self.get_feature_importance()\n",
    "        n_all_importances = len(all_importances)\n",
    "        \n",
    "        plot_importances_dt =\\\n",
    "            all_importances\\\n",
    "            .sort_values()\\\n",
    "            .to_frame('value')\\\n",
    "            .nlargest(top_n_features, 'value')\\\n",
    "            .sort_values('value', ascending=True)\\\n",
    "            .rename_axis('feature')\\\n",
    "            .reset_index()\n",
    "                \n",
    "        if max_scale:\n",
    "            plot_importances_dt['value'] = \\\n",
    "                                plot_importances_dt.value.abs() /\\\n",
    "                                plot_importances_dt.value.abs().max() * 100\n",
    "            \n",
    "        self.plot_importances_dt = plot_importances_dt.copy()\n",
    "        \n",
    "        if len(all_importances) < top_n_features:\n",
    "            title_text = 'All Feature Importances'\n",
    "        else:\n",
    "            title_text = f'Top {top_n_features} (of {n_all_importances}) Feature Importances'       \n",
    "        \n",
    "        if rank_features:\n",
    "            padded_features = \\\n",
    "                plot_importances_dt.feature\\\n",
    "                .str.pad(width=str_pad_width)\\\n",
    "                .values\n",
    "            \n",
    "            ranked_features =\\\n",
    "                plot_importances_dt.index\\\n",
    "                .to_series()\\\n",
    "                .sort_values(ascending=False)\\\n",
    "                .add(1)\\\n",
    "                .astype(str)\\\n",
    "                .str.cat(padded_features, sep='. ')\\\n",
    "                .values\n",
    "\n",
    "            plot_importances_dt['feature'] = ranked_features\n",
    "        \n",
    "        if display_imp_values:\n",
    "            text = plot_importances_dt.value.round(display_imp_value_decimals)\n",
    "        else:\n",
    "            text = None\n",
    "\n",
    "        # create the plot \n",
    "        \n",
    "        fig = px.bar(plot_importances_dt, \n",
    "                     x='value', \n",
    "                     y='feature',\n",
    "                     orientation=orientation, \n",
    "                     width=width, \n",
    "                     height=height,\n",
    "                     text=text)\n",
    "        fig.update_layout(title_text=title_text, title_x=0.5) \n",
    "        fig.update(layout_showlegend=False)\n",
    "        fig.update_yaxes(tickfont=dict(family=yaxes_tickfont_family, \n",
    "                                       size=yaxes_tickfont_size),\n",
    "                         title='')\n",
    "        fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 16.8799,
   "end_time": "2020-09-06T02:15:53.583922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-06T02:15:36.704022",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
