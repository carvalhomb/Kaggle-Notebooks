{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Scikit-Learn ColumnTransformer Dynamically\n",
    "\n",
    "## Using logic to select types of features for transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://images.unsplash.com/photo-1560574188-6a6774965120?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering can be time consuming part of the machine learning process, especially if you are dealing with many features and different types of features. Over the course of my projects, I've developed some heuristics that allow me to construct a reasonably effective Scikit-Learn ColumnTransformer quickly and dynamically. \n",
    "\n",
    "In my post, I will demonstrate 2 techniques. First, I'll show how to select features with logic instead of listing every single column in the code. Second, I will explain the transformer pipelines that I use as my \"defaults\" when training a new model. I will demonstrate my technique on the Ames, IA house prices dataset, which you can find on [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "\n",
    "Before proceeding, I should note that my post assumes that you have worked with Scikit-Learn before and are familiar with how [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), [Pipeline](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.pipeline) & [preprocessing classes](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) facilitate reproducible feature engineering processes. If you need a refresher, checkout this Scikit-Learn [example](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data).\n",
    "\n",
    "Let's start by importing the required packages, classes and functions.\n",
    "\n",
    "\n",
    "\n",
    "<!-- \n",
    " \n",
    "Lastly, I will collapse all the code into a function that will rely on my default settings and instantiate the ColumnTransformer. \n",
    "\n",
    "These rules of thumb work particularly well for tree-based models, which have fewer feature-engineering requirements.\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_log_error\n",
    "import category_encoders as ce\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "DEP_VAR = 'SalePrice'\n",
    "\n",
    "train_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv').set_index('Id')\n",
    "y_train = train_df[DEP_VAR]\n",
    "train_df.drop(DEP_VAR, axis=1, inplace=True)\n",
    "\n",
    "test_df =  pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv').set_index('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "\n",
    "The Ames training dataset has a relatively small number of observations and a decent amount of features at 79. 43 of these features are categorical, and 36 are numeric. I recommend reading [this notebook](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) if you are interested in some exploratory data analysis on the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "\n",
    "feature_types = train_df.dtypes.astype(str).value_counts().to_frame('count').rename_axis('datatype').reset_index()\n",
    "\n",
    "px.bar(feature_types, x='datatype', y='count', color='datatype').update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Features\n",
    "\n",
    "\n",
    "If you are anything like me, the thought of listing 79 features in the code or a configuration file seems like a tedious and unnecessary task. What if there was a way to logically bucket these features by there characteristics?\n",
    "\n",
    "\n",
    "The key insight that allows you to dynamically construct a ColumnTransformer is understanding that there are 3 broad types of features in non-textual, non-time series datasets:\n",
    "\n",
    "1. numerical \n",
    "2. categorical with moderate-to-low cardinality\n",
    "3. categorical with high cardinality\n",
    "\n",
    "Let's take a look at how to dynamically select each feature type and my default transformer pipeline.\n",
    "\n",
    "\n",
    "## Numerical Features\n",
    "\n",
    "The sklearn.compose module does come with a handy class called [make_column_selector](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html), and it provides some limited dynamic functionality to select columns by listing dtypes to include or exclude or by selecting the column names that match a regex pattern. To select numeric features, we will instantiate a function to select columns with the np.number datatype, which will match any integer or float columns. When we call the `select_numeric_features` on the training dataset, we see that it correctly selects the 36 `int64` and `float64` columns.\n",
    "\n",
    "\n",
    "\n",
    "<!-- (Datetime would be a fourth type, but we won't address that here.) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_numeric_features = make_column_selector(dtype_include=np.number)\n",
    "\n",
    "numeric_features = select_numeric_features(train_df)\n",
    "\n",
    "print(f'N numeric_features: {len(numeric_features)} \\n')\n",
    "print(', '.join(numeric_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numeric features, my default transformation involves using the [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html). I impute missing values with median and setting the `add_indicator` parameter to `True`. Using the median instead of the imputer's mean default guards against the influence of outliers. Using the `add_indicator` functionality calls the [MissingIndicator class](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator), which creates missing-indicator columns for each feature with missing values. In my experience, these extra columns can be of moderate importance to the model when the data is not missing at random.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "- When I construct transformer pipelines, I prefer to use the [make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) function as opposed to the Pipeline class. This function replaces the need to explicitly name each Pipeline step and automatically creates the name by taking the lowercased name of the class, e.g. SimpleImputer is named 'simpleimputer'.\n",
    "\n",
    "- Scikit-Learn imputers require that the missing values are represented with np.nan -- hence, my use of the fillna method.\n",
    "\n",
    "- If you are going to use a linear model, you are going to want to insert one of the [preprocessors](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) to center and scale before the imputer.\n",
    "\n",
    "- More sophisticated alternatives to the SimpleImputer include the [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer), which requires centering and scalng, or the experimental [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(np.nan, inplace=True)\n",
    "test_df.fillna(np.nan, inplace=True)\n",
    "\n",
    "numeric_pipeline = make_pipeline(SimpleImputer(strategy='median', add_indicator=True))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical with moderate-to-low cardinality\n",
    "\n",
    "Next, let's discuss how to select and transform the nominal data into numeric form. \n",
    "\n",
    "[One-hot (OH) encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/), where an indicator column is created for each unique value, is the most common method. However, the OH transformation may not be suitable for features with high [cardinality](https://en.wikipedia.org/wiki/Cardinality). OH encoding features with many unique values may create too many columns with very low variance, which may take up too much memory or have a negative impact on the performance of linear models. Hence, we may want to limit the features we select for this encoding to ones below a certain threshold of unique values. For the sake of illustration, I'm going to set my limit at 10 values. In reality, we would probably select the threshold to a higher value depending upon the size of your dataset.\n",
    "\n",
    "Since the [`make_column_selector` isn't capable of detecting cardinality](https://github.com/scikit-learn/scikit-learn/issues/15873), I've developed my own `select_oh_features` custom function. It consists of a piping of pandas methods that do the following:\n",
    "\n",
    "\n",
    "- Selects the `object` and `category` dtypes from the pandas `DataFrame`\n",
    "\n",
    "- Counts the number of unique values for those columns\n",
    "\n",
    "- Subsets the unique value counts if they are less than or equal to `MAX_OH_CARDINALITY`, using an anonymous `lambda` function within the `loc` method\n",
    "\n",
    "- Extracts the column names from the index and returns them as a list\n",
    "\n",
    "\n",
    "When we call the function on the training dataset, we see that it selects 40 of the 43 categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OH_CARDINALITY = 10\n",
    "\n",
    "def select_oh_features(df):\n",
    "    \n",
    "    hc_features =\\\n",
    "        df\\\n",
    "        .select_dtypes(['object', 'category'])\\\n",
    "        .apply(lambda col: col.nunique())\\\n",
    "        .loc[lambda x: x <= MAX_OH_CARDINALITY]\\\n",
    "        .index\\\n",
    "        .tolist()\n",
    "        \n",
    "    return hc_features\n",
    "\n",
    "oh_features = select_oh_features(train_df)\n",
    "\n",
    "print(f'N oh_features: {len(oh_features)} \\n')\n",
    "print(', '.join(oh_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have two default transformations for categorical features with low-to-moderate cardinality: `SimpleImputer` and [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "In the `SimpleImputer`, using the \"constant\" strategy sets the missing values to \"missing_value.\" I don't set the `add_indicator` parameter to `True` since this would create two exactly the same columns. In the OH encoder, I like to set the `handle_unknown` parameter to \"ignore\" instead of using the default \"error,\" so that this transformer won't throw an error if it encounters an unknown value in the test dataset and instead sets all of the OH columns to zero if this situation occurs. Because the Ames test dataset contains categorical values not in the training dataset, our ColumnTransformer will fail on the test dataset without using this setting. If you are planning to use a linear model, you will want to set the `drop` parameter so that the features are not perfectly collinear.\n",
    "\n",
    "\n",
    "<!-- with a \"constant\" strategy -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_pipeline = make_pipeline(SimpleImputer(strategy='constant'), OneHotEncoder(handle_unknown='ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical with high cardinality\n",
    "\n",
    "To select the features with high cardinality, I've created a similar function that selects the `object` and `category` features with unique value counts greater than the threshold. It selects three features that meet these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_hc_features(df):\n",
    "    \n",
    "    hc_features =\\\n",
    "        df\\\n",
    "        .select_dtypes(['object', 'category'])\\\n",
    "        .apply(lambda col: col.nunique())\\\n",
    "        .loc[lambda x: x > MAX_OH_CARDINALITY]\\\n",
    "        .index\\\n",
    "        .tolist()\n",
    "        \n",
    "    return hc_features\n",
    "\n",
    "\n",
    "hc_features = select_hc_features(train_df)\n",
    "\n",
    "print(f'N hc_features: {len(hc_features)} \\n')\n",
    "print(', '.join(hc_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform our features with high cardinality, I could have gone with a more basic approach and used the Scikit-Learn's native [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder) or [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) preprocessor. However, in many cases, these methods are likely to [perform suboptimally](https://towardsdatascience.com/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b) in your model unless you are dealing with ordinal data. I prefer to use the [Category Encoder](http://contrib.scikit-learn.org/category_encoders) package, which has more than a dozen ways of intelligently encoding highly cardinal features. [This post](https://towardsdatascience.com/all-about-categorical-variable-encoding-30) provides an overview of several of these methods. Most of these are supervised techniques, which use the dependent variable to transform the nominal values into numerical ones. The [TargetEncoder](http://contrib.scikit-learn.org/category_encoders/targetencoder.html) is probably the easiest method to understand, but I prefer to use the [Generalized Linear Mixed Model Encoder](http://contrib.scikit-learn.org/category_encoders/glmm.html), which has \"solid statistical theory behind [it]\" and \"no hyperparameters to tune.\" Without diving into the [details of GLMMs](https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models/), at its core, this method encodes the nominal values as the coefficents from a one-hot-encoded linear model. The Category Encoder methods handle missing and unknown values by setting them to zero or the mean of the dependent variable. (If these features in the Ames training dataset had any missing values, we would also want to create missing indicators.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_pipeline = make_pipeline(ce.GLMMEncoder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's put all the pieces together and instantiate our ColumnTransformer:\n",
    "\n",
    "- The `transformer` parameter accepts a list of 3-element tuples. Each tuple contains the name of the transformer/pipeline, the instantiated pipelines and the selector functions that we created. \n",
    "\n",
    "- If you are dealing with a significant number of features and mulit-thread capability, I would definitely set the `n_jobs` parameter, so that the pipelines can be run in parallel.\n",
    "\n",
    "- Lastly, I want to call attention to the `remainder` parameter. By default, ColumnTransformer drops any columns not included in `transformers` list. Alternatively, if you have features that require no transformations, you could set this argument to \"passthrough,\" which will not drop these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(transformers=\\\n",
    "                                       [('numeric_pipeline', numeric_pipeline, select_numeric_features),\\\n",
    "                                        ('oh_pipeline', oh_pipeline, select_oh_features),\\\n",
    "                                        ('hc_pipeline', hc_pipeline, select_hc_features)],\n",
    "                                       n_jobs = multiprocessing.cpu_count(),\n",
    "                                       remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "As you can see, the OH encodings increased the number of columns from 79 to 254. If we hadn't used the `GLMMEncoder`, we would be dealing with over 300 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = column_transformer.fit_transform(train_df, y_train)\n",
    "X_test = column_transformer.transform(test_df)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how are engineered features perform on an GBM regressor with any hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(learning_rate=0.025, n_estimators=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an R-squared value at nearly 0.98, our features explain nearly all of the variation in the training set's dependent variable.\n",
    "The root mean-squared log error was nearly 0.07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R-squared: {r2_score(y_train, y_train_pred)}')\n",
    "print(f'RMSLE: {np.sqrt(mean_squared_log_error(y_train, y_train_pred))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the RMSLE on the test dataset was 0.13249, which means that the model is overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(dict(Id=test_df.index, \n",
    "                               SalePrice=model.predict(X_test)))\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me know if you found this post helpful or have any ideas for improvement. Stay tuned for further posts on training models with Scikit-Learn ColumnTransformers and Pipelines. Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
